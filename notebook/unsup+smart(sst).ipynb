{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9766909,"sourceType":"datasetVersion","datasetId":5981760},{"sourceId":9795814,"sourceType":"datasetVersion","datasetId":6003130},{"sourceId":9804295,"sourceType":"datasetVersion","datasetId":6009338},{"sourceId":183496,"sourceType":"modelInstanceVersion","modelInstanceId":156399,"modelId":178841},{"sourceId":183630,"sourceType":"modelInstanceVersion","modelInstanceId":156514,"modelId":178955}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport unicodedata\n\ndef canonicalize_text(text):\n    text = re.sub(r'[\\d\\W_]+', ' ', text)\n\n    text = ''.join(\n        c for c in unicodedata.normalize('NFD', text)\n        if unicodedata.category(c) != 'Mn'\n    )\n\n    text = text.lower()\n\n    text = text.strip()\n\n    return text","metadata":{"id":"jb0_XMYJ8F2I","trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:38:31.789859Z","iopub.execute_input":"2024-11-30T13:38:31.790314Z","iopub.status.idle":"2024-11-30T13:38:31.822953Z","shell.execute_reply.started":"2024-11-30T13:38:31.790262Z","shell.execute_reply":"2024-11-30T13:38:31.821023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:38:31.826257Z","iopub.execute_input":"2024-11-30T13:38:31.82668Z","iopub.status.idle":"2024-11-30T13:38:37.814896Z","shell.execute_reply.started":"2024-11-30T13:38:31.826631Z","shell.execute_reply":"2024-11-30T13:38:37.81426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nfrom nltk.tree import Tree\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nfrom tqdm import tqdm\n\ndef read_ptb_tree(tree_string):\n    return Tree.fromstring(tree_string)\n\ndef extract_sentence_and_label(tree):\n    label = (tree.label())\n\n    words = tree.leaves()\n    sentence = ' '.join(words)\n\n    return sentence, label\n\ndef read_file(file_path):\n    data = []\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            tree = read_ptb_tree(line.strip())\n            sentence, label = extract_sentence_and_label(tree)\n            data.append({'sentence': sentence, 'label': label})\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:38:37.815909Z","iopub.execute_input":"2024-11-30T13:38:37.816305Z","iopub.status.idle":"2024-11-30T13:38:39.298507Z","shell.execute_reply.started":"2024-11-30T13:38:37.816278Z","shell.execute_reply":"2024-11-30T13:38:39.297874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SST5_Dataset(Dataset):\n    def __init__(self, file_path):\n        self.data = [\n            (\n                tokenizer(\n                    canonicalize_text(row['sentence']),\n                    add_special_tokens=True,\n                    max_length=512,  \n                    padding='max_length', \n                    truncation=True,      \n                    return_tensors=\"pt\"   \n                ), \n                int(row['label']) if isinstance(row['label'], str) else row['label']\n            )\n            for row in read_file(file_path)\n        ]\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        X,y = self.data[idx]\n        input_ids = X['input_ids'].squeeze()\n        attention_mask = X['attention_mask'].squeeze()\n        label = torch.tensor(y, dtype=torch.long)\n        return input_ids, attention_mask, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:38:39.300301Z","iopub.execute_input":"2024-11-30T13:38:39.300752Z","iopub.status.idle":"2024-11-30T13:38:39.307244Z","shell.execute_reply.started":"2024-11-30T13:38:39.300721Z","shell.execute_reply":"2024-11-30T13:38:39.306356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_path = '/kaggle/input/treeset/train.txt'\ntest_path = '/kaggle/input/treeset/test.txt'\nval_path = '/kaggle/input/treeset/dev.txt'\n\ntrainset = SST5_Dataset(train_path)\ntestset = SST5_Dataset(test_path)\nvalset = SST5_Dataset(val_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:38:39.308154Z","iopub.execute_input":"2024-11-30T13:38:39.308381Z","iopub.status.idle":"2024-11-30T13:38:49.848321Z","shell.execute_reply.started":"2024-11-30T13:38:39.308357Z","shell.execute_reply":"2024-11-30T13:38:49.847365Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Sampling data","metadata":{}},{"cell_type":"code","source":"# from torch.utils.data import Subset\n# import random\n\n# train_indices = random.sample(range(len(trainset)), 100)\n# test_indices = random.sample(range(len(testset)), 20)\n# val_indices = random.sample(range(len(valset)), 20)\n\n# trainset = Subset(trainset, train_indices)\n# testset = Subset(testset, test_indices)\n# valset = Subset(valset, val_indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:38:49.849593Z","iopub.execute_input":"2024-11-30T13:38:49.850299Z","iopub.status.idle":"2024-11-30T13:38:49.855683Z","shell.execute_reply.started":"2024-11-30T13:38:49.850259Z","shell.execute_reply":"2024-11-30T13:38:49.854796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install loguru -qU","metadata":{"id":"yuWHYJOZ-8yN","outputId":"8d1232f9-f3a4-40ed-8535-1cc8eec61fdf","trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:38:49.85666Z","iopub.execute_input":"2024-11-30T13:38:49.856949Z","iopub.status.idle":"2024-11-30T13:38:59.463325Z","shell.execute_reply.started":"2024-11-30T13:38:49.856914Z","shell.execute_reply":"2024-11-30T13:38:59.462133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\n\n# # Chọn task cần thực hiện (ở đây chỉ dùng sst)\n# def model_prediction(model, batch, task_name = 'default'):\n#     return {\n#         'default': lambda: model(*batch),\n#         'sst': lambda: model.predict_sentiment(*batch),\n#         # 'para': lambda: model.predict_paraphrase(*batch),\n#         # 'sts': lambda: model.predict_similarity(*batch),\n#     }[task_name]()\n\n# Lớp này kế thừa từ object\n# Đây là định nghĩa hàm regularization được thêm vào để tăng độ mượt model\nclass AdversarialReg(object):\n    def __init__(self, model,\n                epsilon: float = 1e-5,\n                lambda_: float = 1,\n                eta: float = 1e-3,\n                sigma: float = 1e-5,\n                K: int = 1):\n        # gọi hàm khởi tạo của lớp cha\n        super(AdversarialReg, self).__init__()\n        self.embed_backup = {} # từ điển lưu trữ các tham số embedding ban đầu (sau cần khôi phục)\n        self.grad_backup = {} # từ điển lưu trữ các gradient ban đầu (sau khôi phục được)\n        self.model = model\n        self.epsilon = epsilon # mức độ thay đổi các tham số\n        self.lambda_ = lambda_ # hệ số điều chỉnh tính toán hàm mất mát\n        self.eta = eta # learning rate\n        self.sigma = sigma # độ mạnh của noise được thêm vào \n        self.K= K # iterations\n\n    # Lưu các gradient được dùng tính toán (để sau khôi phục lại)\n    def save_gradients(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad: #kiểm tra param có được tính toán trong backpropagation k\n                if param.grad == None: # chưa được tính toán\n                    self.grad_backup[name] = None\n                else:\n                    self.grad_backup[name] = param.grad.clone()\n\n    # lọc những tham số tgia tính toán là emb_name và lưu lại\n    def save_embeddings(self, emb_name):\n        # print(emb_name, type(emb_name))\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and emb_name in name:\n                self.embed_backup[name] = param.data.clone()\n\n    # khôi phục các gradient tính toán \n    def restore_gradient(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                assert name in self.grad_backup #đảm bảo tham số tồn tại\n                param.grad = self.grad_backup[name] #khôi phục giá trị\n\n    # khôi phục những tham số tính toán emb_name\n    def restore_embeddings(self, emb_name): \n        \n        for name, param in self.model.named_parameters():\n            if param.requires_grad and emb_name in name:\n                assert name in self.embed_backup\n                param.data = self.embed_backup[name]\n        self.embed_backup = {}\n\n    #thêm nhiễu vào các tham số\n    def generate_noise(self, emb_name):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and emb_name in name:\n                noise = param.data.new(param.size()).normal_(0, 1) * self.sigma\n                param.data.add_(noise)\n\n    # điều chỉnh tham số nhúng sau khi bị change mà đảm bảo không vượt quá ngưỡng xđ vởi epsilon\n    def project(self, param_name, param_data):\n        change = param_data - self.embed_backup[param_name]\n        change = torch.clamp(change, min = -self.epsilon, max = self.epsilon)\n        return self.embed_backup[param_name] + change\n\n    # điều chỉnh tham số nhúng theo hướng gradient ascent (not GD)\n    def emb_ascent(self, emb_name):\n         for name, param in self.model.named_parameters():\n             if param.requires_grad and emb_name in name:\n                 norm = torch.norm(param.grad, p = float('inf'))\n                 if norm != 0 and not torch.isnan(norm):\n                     param.data.add_(self.eta * (param.grad / norm))\n                     param.data = self.project(name, param.data)\n\n    # Tính toán KL divergence đối xứng -> sử dụng trong quá trình train\n    def symmetric_kl(self, inputs, target):\n        loss = F.kl_div(F.log_softmax(inputs, dim = -1),\n                       F.log_softmax(target, dim = -1),\n                       reduction = 'batchmean',\n                       log_target = True)\n        loss += F.kl_div(F.log_softmax(target, dim = -1),\n                        F.log_softmax(inputs, dim = -1),\n                        reduction = 'batchmean',\n                        log_target = True)\n        return loss\n\n    # Một cách tính toán D_KL tương tự -> dùng cho việc đánh giá hiệu suất (val, test?)\n    def symmetric_kl_check(self, inputs, target, reduce = True):\n        epsilon = 1e-6\n        bs = inputs.size(0)\n        p = F.log_softmax(inputs, 1).exp()\n        y = F.log_softmax(target, 1).exp()\n        rp = -(1.0 / (p + epsilon) - 1 + epsilon).detach().log()\n        ry = -(1.0 / (y + epsilon) - 1 + epsilon).detach().log()\n\n        if reduce:\n            return ((p * (rp - ry) * 2).sum() / bs)\n        else:\n            return ((p * (rp - ry) * 2).sum())\n\n    def max_loss_reg(self, batch, emb_name): #embedding or embedding.\n        emb_name = 'embedding'\n        # print(emb_name, type(emb_name))\n        input_ids, attention_mask = batch\n        self.model.eval() #chuyển model sang chế độ đánh giá, tắt dropout\n        \n        logits = self.model(input_ids, attention_mask)  # tính toán f ban đầu\n        # print(logits, type(logits))\n\n        self.save_gradients()\n        self.save_embeddings(emb_name)\n\n        self.generate_noise(emb_name)\n\n        for _ in range(self.K):\n            self.model.zero_grad()\n\n            adv_logits = self.model(input_ids, attention_mask) # Tính toán f sau khi có thêm nhiễu\n            # print(adv_logits, type(adv_logits))\n            adv_loss = self.symmetric_kl(adv_logits, logits)\n\n            adv_loss.backward()\n            self.emb_ascent(emb_name)\n\n        self.restore_gradient()\n\n        adv_logits = self.model(input_ids, attention_mask) #model_prediction(self.model, batch, task_name)\n        adv_loss = self.symmetric_kl(adv_logits, logits.detach())\n\n        self.restore_embeddings(emb_name)\n        self.model.train()\n\n        return self.lambda_ * adv_loss\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:38:59.465957Z","iopub.execute_input":"2024-11-30T13:38:59.466286Z","iopub.status.idle":"2024-11-30T13:38:59.487402Z","shell.execute_reply.started":"2024-11-30T13:38:59.466256Z","shell.execute_reply":"2024-11-30T13:38:59.486542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Định nghĩa lớp momentum Bregman proximal point để tránh tham số bị cập nhật quá mức\nclass MBPP(object):\n    def __init__(self, model, beta: float = 0.995, mu: float = 1):\n        self.model = model\n        self.beta = beta\n        self.mu = mu\n        self.theta_state = {} #lưu trữ giá trị của tham số tại từng bước cập nhật\n\n        # Cập nhật theta_0 bằng tham số model sau khi pre-trained\n        for name, param in self.model.named_parameters():\n            self.theta_state[name] = param.data.clone()\n\n    # Cập nhật tất cả tham số theo quy tắc \n    def apply_momentum(self, named_parameters):\n        for name, param in self.model.named_parameters():\n            self.theta_state[name] = (1 - self.beta) * param.data.clone() + self.beta * self.theta_state[name]\n\n    # Tính D_Breg\n    def bregman_divergence(self, batch, logits):\n        input_ids, attention_mask = batch\n        # chuyển logits thành xác suất cho từng lớp bằng softmax\n        theta_prob = F.softmax(logits, dim = -1) \n\n        # sao lưu tham số hiện tại của mô hình (param.data) vào param_bak\n        # thay thế bằng tham số lưu ở theta_state để tính toán mà k mất tham số gốc\n        param_bak = {}\n        for name, param in self.model.named_parameters():\n            param_bak[name] = param.data.clone()\n            param.data = self.theta_state[name]\n\n        with torch.no_grad():\n            logits = self.model(input_ids, attention_mask) #model_prediction(self.model, batch, taskname)\n            theta_til_prob = F.softmax(logits, dim = -1).detach()\n\n        # khôi phục lại tham số \n        for name, param in self.model.named_parameters():\n            param.data = param_bak[name]\n\n        #torch.cuda.empty_cache()\n\n        # Tính toán \n        l_s = F.kl_div(theta_prob.log(), theta_til_prob, reduction = 'batchmean') + \\\n        F.kl_div(theta_til_prob.log(), theta_prob, reduction = 'batchmean')\n\n        return self.mu * l_s","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:38:59.488603Z","iopub.execute_input":"2024-11-30T13:38:59.488855Z","iopub.status.idle":"2024-11-30T13:38:59.508433Z","shell.execute_reply.started":"2024-11-30T13:38:59.488831Z","shell.execute_reply":"2024-11-30T13:38:59.507589Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class UnSup_BERT(nn.Module):\n    def __init__(self, bert, is_unsup_train=True):\n        super(UnSup_BERT, self).__init__()\n\n        self.bert = bert\n        self.dropout = nn.Dropout(0.3)\n        self.is_unsup_train = is_unsup_train\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=False)\n        pooled = outputs['pooler_output']\n\n        if not self.is_unsup_train:\n            return pooled\n\n        return self.dropout(pooled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:38:59.511568Z","iopub.execute_input":"2024-11-30T13:38:59.511824Z","iopub.status.idle":"2024-11-30T13:38:59.523061Z","shell.execute_reply.started":"2024-11-30T13:38:59.5118Z","shell.execute_reply":"2024-11-30T13:38:59.522162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass BERT_MLP(nn.Module):\n    def __init__(self, cl_bert, num_labels):\n        super(BERT_MLP, self).__init__()\n        \n        # BERT + supCL\n        self.bert = cl_bert\n\n        # Frozen BERT\n        # for para in self.bert.parameters():\n        #     para.requires_grad = False\n        \n        # MLP\n        self.mlp = nn.Sequential(\n            nn.Linear(768, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(256, num_labels)\n        )\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        logits = self.mlp(outputs)\n        return logits\n\n\nbert = BertModel.from_pretrained('bert-base-uncased')\ncl_bert = UnSup_BERT(bert, is_unsup_train=True)\ncl_bert.load_state_dict(torch.load('/kaggle/input/unsupcse-bert/pytorch/default/1/best_model_uncl.pth'))\n\nmodel = BERT_MLP(cl_bert,num_labels=5)\nmodel.to(device)\n\n\n# model_path = '/kaggle/input/smart/pytorch/default/1/bert-smart-sst-5ep.pth'\n# model.load_state_dict(torch.load(model_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:38:59.524126Z","iopub.execute_input":"2024-11-30T13:38:59.524424Z","iopub.status.idle":"2024-11-30T13:39:05.289999Z","shell.execute_reply.started":"2024-11-30T13:38:59.5244Z","shell.execute_reply":"2024-11-30T13:39:05.289185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from loguru import logger\nfrom torch.cuda.amp import GradScaler, autocast\n\ndef train_one_epoch(model, data_loader, optimizer, device):\n    model.train()\n    total_loss = 0\n\n    scaler = GradScaler()\n    pgd = AdversarialReg(model) \n    mbpp = MBPP(model)\n\n    for input_ids, attention_mask, labels in tqdm(data_loader, desc=\"Training\"):\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        with autocast():\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = F.cross_entropy(outputs, labels)\n        scaler.scale(loss).backward(retain_graph=True)\n\n        # Backpropagation cho adversarial loss\n        with autocast():\n            adv_loss = pgd.max_loss_reg((input_ids, attention_mask), outputs)\n        scaler.scale(adv_loss).backward()\n\n        # Backpropagation cho Bregman divergence\n        with autocast():\n            breg_div = mbpp.bregman_divergence((input_ids, attention_mask), outputs)\n        scaler.scale(breg_div).backward()\n\n        scaler.step(optimizer)  # Thay thế optimizer.step()\n        scaler.update() \n        # optimizer.step()\n        mbpp.apply_momentum(model.named_parameters())\n        total_loss += loss.item()\n        \n    avg_loss = total_loss / len(data_loader)\n    logger.info(f'Training loss: {avg_loss:.4f}')\n    return avg_loss\n\n\n\n# Hàm dùng để tính toán loss ,accuracy của validation và test\ndef eval_one_epoch(model, data_loader, device):\n    model.eval() \n    total_loss = 0\n    correct_predictions = 0\n\n    with torch.no_grad():  \n        for input_ids, attention_mask, labels in tqdm(data_loader, desc=\"Evaluating\"):\n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n            \n            # Tiến hành dự đoán\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = F.cross_entropy(outputs, labels)\n            total_loss += loss.item()\n\n            # Tính số lượng dự đoán chính xác\n            preds = torch.argmax(outputs, dim=1)\n            correct_predictions += torch.sum(preds == labels).item()\n\n    avg_loss = total_loss / len(data_loader)  \n    accuracy = correct_predictions / len(data_loader.dataset)  \n    logger.info(f'Validation loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n    return avg_loss, accuracy\n\n\n# Bước chạy tổng hợp train, val, test\ndef train(model, trainset, valset, device, num_epochs=5, batch_size=8, learning_rate=1e-5, save_dir='/kaggle/working/'):\n    # chuẩn bị sẵn dữ liệu cho từng bước\n    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True) #xáo trộn data (chỉ cần cho train)\n    val_loader = DataLoader(valset, batch_size=batch_size)\n\n    # khởi tạo 1 bộ tối ưu hóa từ thư viện có sẵn\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n    best_val_loss = float('inf')\n    best_epoch = 0\n\n    best_model_path = f\"{save_dir}best_model.pth\"\n    \n    # Đánh giá quá từng epoch\n    for epoch in range(num_epochs):\n        torch.cuda.empty_cache()\n        logger.info(f'Epoch {epoch + 1}/{num_epochs}')\n        \n        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n\n        val_loss, val_accuracy = eval_one_epoch(model, val_loader, device)\n\n\n        if val_loss < best_val_loss:\n            logger.info(f\"Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}. Saving model...\")\n            best_val_loss = val_loss\n            best_epoch = epoch\n            \n            torch.save(model.state_dict(), best_model_path)\n            logger.info(f\"Model saved to {best_model_path}\")\n            \n    logger.info(f\"Training completed. Best validation loss: {best_val_loss:.4f} at epoch {best_epoch + 1}.\")\n        \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:39:05.291259Z","iopub.execute_input":"2024-11-30T13:39:05.291645Z","iopub.status.idle":"2024-11-30T13:39:05.31953Z","shell.execute_reply.started":"2024-11-30T13:39:05.291619Z","shell.execute_reply":"2024-11-30T13:39:05.318663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()\ntrain(model, trainset, valset, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:39:05.320484Z","iopub.execute_input":"2024-11-30T13:39:05.320768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport numpy as np\n\ndef test_model(model_path, testset, device, batch_size=16):\n\n    model = BERT_MLP(cl_bert, num_labels=5)\n    model.load_state_dict(torch.load(model_path))\n    model.eval()\n    model.to(device)\n    \n    test_loader = DataLoader(testset, batch_size=batch_size)\n    \n    all_preds = []\n    all_labels = []\n    total_loss = 0\n    correct_predictions = 0\n    \n    with torch.no_grad():\n        for input_ids, attention_mask, labels in tqdm(test_loader, desc=\"Testing\"):\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = F.cross_entropy(outputs, labels)\n            total_loss += loss.item()\n            \n            # Get predictions\n            preds = torch.argmax(outputs, dim=1)\n            correct_predictions += torch.sum(preds == labels).item()\n            \n            # Store predictions and labels for metric calculation\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Calculate metrics\n    accuracy = correct_predictions / len(testset)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        all_labels, \n        all_preds, \n        average='weighted'\n    )\n    avg_loss = total_loss / len(test_loader)\n    \n    # Print metrics\n    logger.info(f\"Test Results:\")\n    logger.info(f\"Average Loss: {avg_loss:.4f}\")\n    logger.info(f\"Accuracy: {accuracy:.4f}\")\n    logger.info(f\"Precision: {precision:.4f}\")\n    logger.info(f\"Recall: {recall:.4f}\")\n    logger.info(f\"F1 Score: {f1:.4f}\")\n    \n    # Calculate and plot confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(\n        cm, \n        annot=True, \n        fmt='d', \n        cmap='Blues',\n        xticklabels=np.unique(all_labels),\n        yticklabels=np.unique(all_labels)\n    )\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.tight_layout()\n    plt.show()\n    \n    metrics = {\n        'loss': avg_loss,\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'confusion_matrix': cm\n    }\n    \n    return metrics\n\nmodel_path = '/kaggle/working/best_model.pth'\n\nmetrics = test_model(\n    model_path=model_path,\n    testset=testset,\n    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}